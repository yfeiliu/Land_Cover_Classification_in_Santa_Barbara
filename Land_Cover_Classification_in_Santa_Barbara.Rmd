---
title: '**Land Cover Classification in Santa Barbara: A Decision Tree Approach**'
author: "Yifei Liu"
date: '2023-11-30'
output:
  html_document: default
---
## Background
Monitoring the distribution and change in land cover types can help us understand the impacts of phenomena like climate change, natural disasters, deforestation, and urbanization. Determining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance. 

Classifying remotely sensed imagery into landcover classes enables us to understand the distribution and change in landcover types over large areas. There are many approaches for performing landcover classification -- *supervised* approaches use training data labeled by the user, whereas *unsupervised* approaches use algorithms to create groups which are identified by the user afterward.\

In this project, we are using a form of supervised classification, a *decision tree classifier*. [Decision trees](https://medium.com/@ml.at.berkeley/machine-learning-crash-course-part-5-decision-trees-and-ensemble-models-dcc5a36af8cd) classify pixels using a series of conditions based on values in spectral bands. These conditions (or decisions) are developed based on training data. 

**Credit**: this project is based on a materials developed by Chris Kibler.

## Project Description
This project focuses on classifying land cover in Southern Santa Barbara County using Landsat 5 Thematic Mapper imagery. Utilizing a supervised classification approach, specifically a decision tree classifier, we classify pixels into distinct land cover types based on their spectral reflectance characteristics. The land cover types identified include green vegetation, dry grass or soil, urban areas, and water bodies. This approach provides insights into the distribution and dynamics of land cover, crucial for understanding environmental changes and urbanization impacts. 

## Skills Demonstrated

- **Remote Sensing Analysis**: Utilizing Landsat imagery to study environmental features.
- **Spatial Data Handling**: Proficient use of `sf` and `terra` packages in R for managing and analyzing spatial data.
- **Supervised Machine Learning**: Implementing a decision tree classifier to categorize land cover types, demonstrating an understanding of machine learning concepts in a practical context.
- **Data Processing and Wrangling**: Employing `dplyr` and other `tidyverse` packages for efficient data manipulation and preparation.
- **Visualization**: Creating informative visualizations using `tmap` and `ggplot2` to effectively communicate the results of the analysis.
- **Critical Thinking and Problem-Solving**: Developing a systematic approach to address the challenge of classifying land cover from satellite imagery.


## Materials and Methods
### Data Source
#### [Landsat 5](https://www.usgs.gov/landsat-missions/landsat-5) Thematic Mapper Imargery

-   **Date**: September 25, 2007
-   **Bands Used**: 1, 2, 3, 4, 5, 7
-   **Product**: Collection 2 surface reflectance

#### Study Area and Training Data

-   **Region**: Southern Santa Barbara county
-   **Training Data**: Polygons representing training sites categorized into four land cover types

### Methodology
1. **Data Preparation**: Loading and processing Landsat 5 imagery.
2. **Study Area Definition**: Defining and masking the study area.
3. **Reflectance Conversion**: Adjusting Landsat values to reflectance.
4. **Training Data Extraction**: Extracting spectral data for land cover types.
5. **Decision Tree Training**: Implementing the decision tree classifier.
6. **Classification and Visualization**: Applying the classifier and visualizing results.


## Analysis Workflow
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Initial Setup
#### Load Essential Libraries
Our project requires a variety of tools and functions, each served by specific R libraries. We'll be handling both vector and raster data, necessitating the use of `sf` for spatial data frames and `terra` for raster operations. For the classification algorithm and its visualization, we rely on `rpart`, `rpart.plot`, and `tmap`.

```{r initial-setup, include=TRUE, message=FALSE, warning=FALSE}
# Loading libraries
library(sf)
library(terra)
library(here)
library(dplyr)
library(rpart)
library(rpart.plot)
library(tmap)
```

#### Setting Working Directory
We will set the working directory to the folder that holds the data for this project. It's important to note that file paths may vary depending on the user's directory structure.

```{r set-working-directory, include=TRUE, message=FALSE, warning=FALSE}
# Clearing workspace
rm(list = ls())

# Setting the working directory
here::i_am("Land_Cover_Classification_in_Santa_Barbara.Rmd")
setwd(here())
```

### Data Processing and Preparation
#### Loading and Inspecting Landsat Imagery
In this section, our primary focus is to prepare the Landsat 5 Thematic Mapper imagery for analysis. Here's how we approach the data preparation:

1. **Creating a Raster Stack**: We begin by constructing a raster stack from six different spectral bands of the Landsat imagery. Each band, identified by its file name (such as `B1.tif` for band 1), captures unique spectral characteristics of the Earth's surface.\
**Note**: Band 6, corresponding to thermal data, is excluded from our analysis as it does not serve our current purpose.

2. **Reading the Data**: Utilizing the `rast` function from the `terra` package, we efficiently load all the necessary band files into a single raster stack. This method streamlines the process, allowing for simultaneous handling of multiple bands.

3. **Renaming Layers**: For better clarity and ease of reference, we update the layer names within our raster stack to correspond to each spectral band (e.g., blue, green, red, NIR, SWIR1, SWIR2).

4. **Visual Inspection**: To get a preliminary view of the area of interest, we generate a true-color image from the stack. This step is crucial for visually assessing the data quality and understanding the landscape we are studying.

Here's how this process is implemented in R:
```{r load-inspect-landsat, include=TRUE}
# Listing Landsat data files for each band, including the full file path
filelist <- list.files("./data/landsat-data/", full.names = TRUE)

# Creating a raster stack from the listed files
landsat_20070925 <- rast(filelist)

# Updating layer names to correspond to Landsat spectral bands
names(landsat_20070925) <- c("blue", "green", "red", "NIR", "SWIR1", "SWIR2")

# Plotting a true color image for initial inspection
plotRGB(landsat_20070925, r = 3, g = 2, b = 1, stretch = "lin")
```

#### Preparing Study Area
After preparing the Landsat imagery, the next critical step is to define the specific geographic area for our analysis. For this project, we concentrate on the southern part of Santa Barbara County, an area selected based on the availability of relevant training data. 

1. **Loading Geographic Boundaries**: We load a shapefile that precisely delineates the southern portion of Santa Barbara County. This shapefile acts as a geographical boundary, helping us to focus our analysis on the area of interest.

2. **Aligning Data Projections**: Ensuring consistency in spatial reference systems is crucial. Therefore, we transform the geographic boundary layer to match the coordinate reference system (CRS) of the Landsat data. This step is essential to avoid any spatial misalignment during the analysis.

The R code for this process is as follows:
```{r prepare-study-area, include=TRUE}
# Loading the shapefile defining the study area
SB_county_south <- st_read("./data/SB_county_south.shp", quiet = TRUE)

# Aligning the CRS with the Landsat imagery
SB_county_south <- st_transform(SB_county_south, crs = crs(landsat_20070925))
```

#### Cropping and Masking Landsat Data to Study Area
With the study area defined, our next task is to tailor the Landsat data specifically to this region. This involves two key processes - cropping and masking - which help in refining our dataset for efficient analysis.

1. **Cropping to the Study Area**: We start by cropping the Landsat imagery to the boundaries of the southern Santa Barbara County. Cropping is an effective way to eliminate unnecessary data outside our area of interest, thus enhancing processing efficiency.

2. **Masking the Data**: Following the cropping, we apply a mask to the Landsat data using the same shapefile. Masking helps in further refining the dataset by ensuring that only the land within the defined boundaries is included in our analysis. This step is particularly important for focusing solely on the geographic area relevant to our study.

3. **Optimizing the Working Environment**: To maintain an efficient working environment, we remove any superfluous data objects from our workspace. This practice is not only good for memory management but also keeps our workspace organized.

```{r crop-mask-landsat, include=TRUE}
# Cropping Landsat data to the extent of our study area
landsat_cropped <- crop(landsat_20070925, SB_county_south)

# Applying a mask to focus on the area within the southern SB county
landsat_masked <- mask(landsat_cropped, SB_county_south)

# Cleaning up the workspace by removing no longer needed objects
rm(landsat_20070925, SB_county_south, landsat_cropped)
```

#### Converting Landsat Values to Reflectance Values

An essential step in our land cover classification project is converting the raw pixel values of our Landsat data into actual reflectance values. This conversion is crucial for ensuring the accuracy of our analysis, as reflectance values offer a more reliable representation of the Earth's surface characteristics.

1. **Applying Scaling Factors**: For [Landsat Collection 2](https://www.usgs.gov/landsat-missions/landsat-collection-2) data, specific scaling factors are provided to convert raw pixel values into reflectance. We follow the guidelines provided by the USGS, which specify a valid pixel value range (7,273-43,636) and [scaling factors](https://www.usgs.gov/faqs/how-do-i-use-scale-factor-landsat-level-2-science-products#:~:text=Landsat%20Collection%202%20surface%20temperature,the%20scale%20factor%20is%20applied.) (multiplicative: 0.0000275, additive: -0.2) for this conversion.

2. **Data Cleaning and Conversion Process**: 
    - **Cleaning Erroneous Values**: We first identify and reclassify any erroneous pixel values as `NA` (not applicable), ensuring that only valid data is used for conversion.
    - **Applying Scale Factors**: We then adjust the values of each pixel based on the provided scaling factors. This step recalibrates the data, making the pixel values range between 0 and 100%, reflecting the actual proportion of light being reflected.

The R code for this conversion process is as below:

```{r convert-reflectance, include=TRUE, warning=FALSE}
# Reclassifying erroneous pixel values
rcl <- matrix(c(-Inf, 7273, NA,
                 43636, Inf, NA), ncol = 3, byrow = TRUE)

landsat <- classify(landsat_masked, rcl = rcl)

# Adjusting values based on Landsat Collection 2 scaling factors
landsat <- (landsat * 0.0000275 - 0.2) * 100

# Plotting the data for a quick visual check
plotRGB(landsat, r = 3, g = 2, b = 1, stretch = "lin")

# Checking values are 0 - 100
summary(landsat)
```


### Image Classification
#### Extracting Training Data for Supervised Classification

Before diving into the classification of our Landsat imagery, we first need to establish a reliable set of training data. Training data acts as a guide, helping our classification algorithm learn how to differentiate between various land cover types based on spectral values.

1. **Loading Land Cover Training Locations**: Our first step is to load a shapefile that marks specific locations in the study area, each tagged with one of our four predefined land cover types (green vegetation, soil/dead grass, urban, and water).

2. **Aligning Training Data with Landsat Imagery**: We transform the training data to match the CRS of our Landsat dataset, ensuring spatial consistency.

3. **Extracting Spectral Values**: The spectral values at each training location are extracted from the Landsat data. These values are crucial as they provide the spectral signature for each land cover type.

4. **Creating a Comprehensive Training Dataset**: We combine the spatial data of the training locations with their extracted spectral values. This comprehensive dataset correlates land cover types with their corresponding spectral characteristics, laying the groundwork for our supervised classification.

Here's the R code for extracting and preparing the training data:
```{r extract-training-data, include=TRUE}
# Loading and transforming training data to align with Landsat CRS
training_data <- st_read("./data/trainingdata.shp", quiet = TRUE) %>%
  st_transform(., crs = crs(landsat))

# Extracting spectral reflectance values at training sites
training_data_values <- extract(landsat, training_data, df = TRUE)

# Converting training data to data frame
training_data_attributes <- training_data %>%
  st_drop_geometry()

# Joining training data attributes and extracted reflectance values
SB_training_data <- left_join(training_data_values, training_data_attributes,
                              by = c("ID" = "id")) %>%
  mutate(type = as.factor(type)) # Converting landcover type to factor
```

#### Training the Decision Tree Classifier
In our quest to classify land cover types accurately, we utilize a decision tree classifier. This section outlines the steps taken to train this classifier using our prepared training dataset.

1. **Establishing the Model Formula**: The cornerstone of training our classifier is defining the model formula. This formula specifies our response variable (land cover types) and predictor variables (spectral bands from the Landsat data).

2. **Implementing the CART Algorithm**: We employ the `rpart` function in R, which implements the Classification and Regression Trees ([CART](https://medium.com/geekculture/decision-trees-with-cart-algorithm-7e179acee8ff)) algorithm. This algorithm is highly effective in creating decision trees for classification tasks.

3. **Configuring the Decision Tree Training**: 
    - **Method**: We set the method to "class" (`method = "class"`) to indicate that we are performing classification.
    - **Handling Missing Values**: We configure the function to omit any NA values (`na.action = na.omit`) in the analysis, ensuring the integrity of our model.

4. **Training the Model**: The decision tree is trained using our model formula and the training dataset. This process involves the algorithm learning to differentiate between land cover types based on the spectral values.

5. **Visualizing the Decision Tree**: Post-training, we visualize the decision tree to understand its structure. The tree comprises a series of binary decisions based on spectral band values, each leading to a classification outcome.

Here's how we implement these steps in R:

```{r train-decision-tree, include=TRUE}
# Defining the model formula
SB_formula <- type ~ red + green + blue + NIR + SWIR1 + SWIR2

# Training the decision tree classifier
SB_decision_tree <- rpart(formula = SB_formula,
                          data = SB_training_data,
                          method = "class",
                          na.action = na.omit)

# Visualizing the trained decision tree
prp(SB_decision_tree)
```

#### Applying Decision Tree to Entire Image

With our decision tree classifier finely tuned, the next step is to apply this model across the entire Landsat image. This step transforms our classifier from a theoretical model into a practical tool, capable of classifying each pixel of the image into one of our predefined land cover types.

1. **Utilizing the `predict()` Function**: The `terra` package in R provides the `predict()` function, an essential tool that enables us to apply the trained decision tree model to the Landsat dataset. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The `predict()` function will return a raster layer with integer values. 

2. **Interpreting the Classification Results**: To understand what each integer in the output raster represents, we refer to the *factor levels* in our training data. This step is crucial for translating the numeric output into meaningful land cover categories.

Here's the implementation in R:
```{r apply-decision-tree, include=TRUE}
# Applying the decision tree model to the Landsat image
SB_classification <- predict(landsat, SB_decision_tree, type = "class", na.rm = TRUE)

# Understanding the classification output by inspecting factor levels
levels(SB_training_data$type)

```

### Visualization
The final step in our project is the visualization of the land cover classification. This is where we get to see the results of our analysis, translated into a vivid land cover map of the study area!

```{r visualize-land-cover, message=FALSE, warning=FALSE}
# plot results

tm_shape(SB_classification) +
  tm_raster(col.scale = tm_scale_categorical(values = c("#8DB580", "#F2DDA4", "#7E8987", "#6A8EAE")),
            col.legend = tm_legend(labels = c("green vegetation", "soil/dead grass", "urban", "water"),
                                   title = "Landcover type")) +
  tm_layout(legend.position = c("left", "bottom"))

```
